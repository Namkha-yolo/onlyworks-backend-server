const { GoogleGenerativeAI } = require('@google/generative-ai');
const fs = require('fs').promises;
const path = require('path');
const { ApiError } = require('../middleware/errorHandler');
const { logger } = require('../utils/logger');

/**
 * Comprehensive AI Analysis Backtesting Service
 * Tests AI analysis accuracy against ground truth data and validates model performance
 */
class AIAnalysisBacktestService {
  constructor() {
    this.genAI = process.env.GOOGLE_AI_API_KEY
      ? new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY)
      : null;

    this.model = this.genAI ? this.genAI.getGenerativeModel({ model: "gemini-1.5-flash" }) : null;

    // Performance metrics tracking
    this.backtestResults = new Map(); // testId -> results
    this.performanceMetrics = {
      totalTests: 0,
      passedTests: 0,
      averageAccuracy: 0,
      modelLatencies: [],
      confidenceScores: [],
      errorRates: {
        activity_detection: 0,
        productivity_scoring: 0,
        app_detection: 0,
        blocker_identification: 0
      }
    };
  }

  /**
   * Run comprehensive backtesting against known ground truth data
   */
  async runBacktest(options = {}) {
    const {
      testDataPath = './test-data/screenshots',
      groundTruthPath = './test-data/ground-truth.json',
      testSampleSize = 50,
      enableRealTimeValidation = true,
      models = ['gemini-1.5-flash', 'gemini-1.0-pro'],
      confidenceThreshold = 0.7
    } = options;

    logger.info('Starting comprehensive AI analysis backtest', {
      testSampleSize,
      models: models.length,
      confidenceThreshold
    });

    const backtestId = `backtest-${Date.now()}`;
    const startTime = Date.now();

    try {
      // Load ground truth data
      const groundTruthData = await this.loadGroundTruthData(groundTruthPath);

      // Generate test samples
      const testSamples = await this.generateTestSamples(groundTruthData, testSampleSize);

      const results = {
        backtest_id: backtestId,
        started_at: new Date().toISOString(),
        test_configuration: {
          sample_size: testSampleSize,
          models_tested: models,
          confidence_threshold: confidenceThreshold,
          real_time_validation: enableRealTimeValidation
        },
        model_results: {},
        overall_metrics: {},
        detailed_analysis: {},
        validation_errors: [],
        performance_summary: {}
      };

      // Test each model
      for (const modelName of models) {
        logger.info(`Testing model: ${modelName}`);
        const modelResults = await this.testModel(modelName, testSamples, groundTruthData);
        results.model_results[modelName] = modelResults;
      }

      // Calculate comparative metrics
      results.overall_metrics = await this.calculateOverallMetrics(results.model_results);
      results.detailed_analysis = await this.generateDetailedAnalysis(results.model_results, testSamples);

      // Real-time validation if enabled
      if (enableRealTimeValidation) {
        results.real_time_validation = await this.performRealTimeValidation(testSamples.slice(0, 10));
      }

      // Generate performance insights
      results.performance_summary = await this.generatePerformanceInsights(results);

      const duration = Date.now() - startTime;
      results.completed_at = new Date().toISOString();
      results.total_duration_ms = duration;

      // Store results
      this.backtestResults.set(backtestId, results);

      logger.info('Backtest completed successfully', {
        backtest_id: backtestId,
        duration_ms: duration,
        total_tests: testSampleSize * models.length,
        overall_accuracy: results.overall_metrics.average_accuracy
      });

      return {
        success: true,
        data: results
      };

    } catch (error) {
      logger.error('Backtest failed', {
        backtest_id: backtestId,
        error: error.message
      });

      return {
        success: false,
        error: {
          code: 'BACKTEST_FAILED',
          message: 'AI analysis backtest failed',
          details: error.message
        }
      };
    }
  }

  /**
   * Test a specific model against test samples
   */
  async testModel(modelName, testSamples, groundTruth) {
    const modelStartTime = Date.now();
    const results = {
      model_name: modelName,
      tests_run: testSamples.length,
      accuracy_metrics: {
        activity_detection: { correct: 0, total: 0, accuracy: 0 },
        productivity_scoring: { mae: 0, rmse: 0, correlation: 0 },
        app_detection: { correct: 0, total: 0, accuracy: 0 },
        blocker_identification: { precision: 0, recall: 0, f1: 0 }
      },
      performance_metrics: {
        average_latency_ms: 0,
        min_latency_ms: Number.MAX_VALUE,
        max_latency_ms: 0,
        total_tokens_used: 0,
        average_confidence: 0
      },
      test_results: []
    };

    const latencies = [];
    const confidenceScores = [];
    const activityPredictions = [];
    const activityGroundTruth = [];
    const productivityPredictions = [];
    const productivityGroundTruth = [];

    for (const [index, sample] of testSamples.entries()) {
      const testStartTime = Date.now();

      try {
        // Generate AI analysis (mock for now - would use actual image analysis)
        const analysisResult = await this.generateMockAnalysis(sample, modelName);
        const latency = Date.now() - testStartTime;
        latencies.push(latency);
        confidenceScores.push(analysisResult.confidence_score);

        // Compare with ground truth
        const comparison = this.compareWithGroundTruth(analysisResult, groundTruth[sample.id]);

        results.test_results.push({
          test_id: sample.id,
          predicted: analysisResult,
          ground_truth: groundTruth[sample.id],
          accuracy_scores: comparison,
          latency_ms: latency,
          confidence: analysisResult.confidence_score
        });

        // Collect metrics
        activityPredictions.push(analysisResult.activity_detected);
        activityGroundTruth.push(groundTruth[sample.id].activity_detected);
        productivityPredictions.push(analysisResult.productivity_score);
        productivityGroundTruth.push(groundTruth[sample.id].productivity_score);

        // Update accuracy counts
        if (comparison.activity_match) {
          results.accuracy_metrics.activity_detection.correct++;
        }
        results.accuracy_metrics.activity_detection.total++;

        if (comparison.app_match) {
          results.accuracy_metrics.app_detection.correct++;
        }
        results.accuracy_metrics.app_detection.total++;

      } catch (error) {
        logger.error(`Test failed for sample ${sample.id}`, { error: error.message });
        results.test_results.push({
          test_id: sample.id,
          error: error.message,
          failed: true
        });
      }
    }

    // Calculate final metrics
    results.accuracy_metrics.activity_detection.accuracy =
      results.accuracy_metrics.activity_detection.correct / results.accuracy_metrics.activity_detection.total;

    results.accuracy_metrics.app_detection.accuracy =
      results.accuracy_metrics.app_detection.correct / results.accuracy_metrics.app_detection.total;

    results.accuracy_metrics.productivity_scoring.mae =
      this.calculateMAE(productivityPredictions, productivityGroundTruth);

    results.accuracy_metrics.productivity_scoring.rmse =
      this.calculateRMSE(productivityPredictions, productivityGroundTruth);

    results.performance_metrics.average_latency_ms = latencies.reduce((a, b) => a + b, 0) / latencies.length;
    results.performance_metrics.min_latency_ms = Math.min(...latencies);
    results.performance_metrics.max_latency_ms = Math.max(...latencies);
    results.performance_metrics.average_confidence = confidenceScores.reduce((a, b) => a + b, 0) / confidenceScores.length;

    const modelDuration = Date.now() - modelStartTime;
    results.total_test_duration_ms = modelDuration;

    return results;
  }

  /**
   * Generate detailed analysis of test results
   */
  async generateDetailedAnalysis(modelResults, testSamples) {
    const analysis = {
      model_comparison: {},
      error_analysis: {},
      confidence_analysis: {},
      performance_trends: {},
      recommendations: []
    };

    // Compare models
    const modelNames = Object.keys(modelResults);
    if (modelNames.length > 1) {
      analysis.model_comparison = {
        best_accuracy_model: null,
        best_performance_model: null,
        accuracy_differences: {},
        performance_differences: {}
      };

      let bestAccuracy = 0;
      let bestPerformance = Number.MAX_VALUE;

      modelNames.forEach(modelName => {
        const result = modelResults[modelName];
        const avgAccuracy = (
          result.accuracy_metrics.activity_detection.accuracy +
          result.accuracy_metrics.app_detection.accuracy
        ) / 2;

        if (avgAccuracy > bestAccuracy) {
          bestAccuracy = avgAccuracy;
          analysis.model_comparison.best_accuracy_model = modelName;
        }

        if (result.performance_metrics.average_latency_ms < bestPerformance) {
          bestPerformance = result.performance_metrics.average_latency_ms;
          analysis.model_comparison.best_performance_model = modelName;
        }
      });
    }

    // Error pattern analysis
    analysis.error_analysis = await this.analyzeErrorPatterns(modelResults);

    // Confidence correlation analysis
    analysis.confidence_analysis = await this.analyzeConfidenceCorrelation(modelResults);

    // Generate recommendations
    analysis.recommendations = await this.generateRecommendations(modelResults, analysis);

    return analysis;
  }

  /**
   * Analyze error patterns across test results
   */
  async analyzeErrorPatterns(modelResults) {
    const errorPatterns = {
      common_failure_scenarios: [],
      confidence_vs_accuracy: {},
      temporal_patterns: {},
      app_specific_errors: {}
    };

    Object.entries(modelResults).forEach(([modelName, results]) => {
      const failedTests = results.test_results.filter(test => !test.failed && test.accuracy_scores);

      // Group by confidence ranges
      const confidenceRanges = { low: [], medium: [], high: [] };

      failedTests.forEach(test => {
        if (test.confidence < 0.3) confidenceRanges.low.push(test);
        else if (test.confidence < 0.7) confidenceRanges.medium.push(test);
        else confidenceRanges.high.push(test);
      });

      errorPatterns.confidence_vs_accuracy[modelName] = {
        low_confidence: this.calculateAverageAccuracy(confidenceRanges.low),
        medium_confidence: this.calculateAverageAccuracy(confidenceRanges.medium),
        high_confidence: this.calculateAverageAccuracy(confidenceRanges.high)
      };
    });

    return errorPatterns;
  }

  /**
   * Generate performance improvement recommendations
   */
  async generateRecommendations(modelResults, analysis) {
    const recommendations = [];

    // Check accuracy thresholds
    Object.entries(modelResults).forEach(([modelName, results]) => {
      const avgAccuracy = (
        results.accuracy_metrics.activity_detection.accuracy +
        results.accuracy_metrics.app_detection.accuracy
      ) / 2;

      if (avgAccuracy < 0.8) {
        recommendations.push({
          type: 'accuracy_improvement',
          model: modelName,
          priority: 'high',
          message: `Model accuracy (${(avgAccuracy * 100).toFixed(1)}%) below 80% threshold`,
          suggested_actions: [
            'Increase training data diversity',
            'Implement ensemble methods',
            'Fine-tune confidence thresholds'
          ]
        });
      }

      if (results.performance_metrics.average_latency_ms > 2000) {
        recommendations.push({
          type: 'performance_optimization',
          model: modelName,
          priority: 'medium',
          message: `Average latency (${results.performance_metrics.average_latency_ms}ms) exceeds 2s threshold`,
          suggested_actions: [
            'Consider model compression',
            'Implement request batching',
            'Use faster model variants'
          ]
        });
      }
    });

    return recommendations;
  }

  /**
   * Generate mock analysis for testing (would be replaced with actual AI analysis)
   */
  async generateMockAnalysis(sample, modelName) {
    // Simulate model-specific variations
    const baseAccuracy = modelName === 'gemini-1.5-flash' ? 0.85 : 0.80;
    const variation = (Math.random() - 0.5) * 0.3; // Â±15% variation

    const activities = [
      'coding', 'writing', 'research', 'communication', 'design',
      'meeting', 'documentation', 'testing', 'debugging', 'planning'
    ];

    const apps = [
      'Visual Studio Code', 'Chrome', 'Slack', 'Figma', 'Terminal',
      'Notion', 'Zoom', 'Postman', 'GitHub', 'Discord'
    ];

    return {
      activity_detected: activities[Math.floor(Math.random() * activities.length)],
      productivity_score: Math.max(0, Math.min(100, 70 + variation * 100)),
      confidence_score: Math.max(0.1, Math.min(1.0, baseAccuracy + variation)),
      detected_apps: [
        {
          name: apps[Math.floor(Math.random() * apps.length)],
          confidence: 0.8 + Math.random() * 0.2
        }
      ],
      is_blocked: Math.random() < 0.2,
      blocker_type: Math.random() < 0.2 ? 'social_media' : null,
      model_version: modelName,
      processing_time_ms: 150 + Math.random() * 300
    };
  }

  /**
   * Compare AI prediction with ground truth
   */
  compareWithGroundTruth(predicted, groundTruth) {
    return {
      activity_match: predicted.activity_detected === groundTruth.activity_detected,
      productivity_score_diff: Math.abs(predicted.productivity_score - groundTruth.productivity_score),
      app_match: this.checkAppMatch(predicted.detected_apps, groundTruth.detected_apps),
      blocker_match: predicted.is_blocked === groundTruth.is_blocked,
      overall_score: this.calculateOverallAccuracyScore(predicted, groundTruth)
    };
  }

  /**
   * Load ground truth data for testing
   */
  async loadGroundTruthData(groundTruthPath) {
    // Mock ground truth data for demonstration
    const mockGroundTruth = {};

    for (let i = 1; i <= 100; i++) {
      mockGroundTruth[`test-sample-${i}`] = {
        activity_detected: ['coding', 'writing', 'research'][Math.floor(Math.random() * 3)],
        productivity_score: 50 + Math.random() * 50,
        detected_apps: [{ name: 'Visual Studio Code', confidence: 0.9 }],
        is_blocked: Math.random() < 0.2,
        user_verified: true
      };
    }

    return mockGroundTruth;
  }

  /**
   * Generate test samples
   */
  async generateTestSamples(groundTruthData, sampleSize) {
    const samples = [];
    const groundTruthKeys = Object.keys(groundTruthData);

    for (let i = 0; i < sampleSize; i++) {
      const randomKey = groundTruthKeys[Math.floor(Math.random() * groundTruthKeys.length)];
      samples.push({
        id: randomKey,
        screenshot_path: `/test-data/screenshots/${randomKey}.png`,
        timestamp: new Date().toISOString(),
        metadata: {
          window_title: 'Test Application',
          active_app: 'Test App'
        }
      });
    }

    return samples;
  }

  /**
   * Utility functions for metrics calculation
   */
  calculateMAE(predictions, groundTruth) {
    const errors = predictions.map((pred, i) => Math.abs(pred - groundTruth[i]));
    return errors.reduce((a, b) => a + b, 0) / errors.length;
  }

  calculateRMSE(predictions, groundTruth) {
    const squaredErrors = predictions.map((pred, i) => Math.pow(pred - groundTruth[i], 2));
    return Math.sqrt(squaredErrors.reduce((a, b) => a + b, 0) / squaredErrors.length);
  }

  checkAppMatch(predictedApps, groundTruthApps) {
    if (!predictedApps.length || !groundTruthApps.length) return false;
    return predictedApps.some(predApp =>
      groundTruthApps.some(gtApp => gtApp.name === predApp.name)
    );
  }

  calculateOverallAccuracyScore(predicted, groundTruth) {
    let score = 0;
    let total = 0;

    // Activity detection (30% weight)
    if (predicted.activity_detected === groundTruth.activity_detected) score += 30;
    total += 30;

    // Productivity score (25% weight) - inverse of normalized difference
    const productivityAccuracy = 1 - (Math.abs(predicted.productivity_score - groundTruth.productivity_score) / 100);
    score += productivityAccuracy * 25;
    total += 25;

    // App detection (25% weight)
    if (this.checkAppMatch(predicted.detected_apps, groundTruth.detected_apps)) score += 25;
    total += 25;

    // Blocker detection (20% weight)
    if (predicted.is_blocked === groundTruth.is_blocked) score += 20;
    total += 20;

    return score / total;
  }

  calculateAverageAccuracy(tests) {
    if (!tests.length) return 0;
    const accuracies = tests.map(test => test.accuracy_scores.overall_score);
    return accuracies.reduce((a, b) => a + b, 0) / accuracies.length;
  }

  async calculateOverallMetrics(modelResults) {
    const models = Object.keys(modelResults);
    const metrics = {
      total_tests: 0,
      average_accuracy: 0,
      average_latency: 0,
      best_performing_model: null,
      model_rankings: []
    };

    let totalAccuracy = 0;
    let totalLatency = 0;
    let totalTests = 0;

    const modelPerformance = models.map(modelName => {
      const result = modelResults[modelName];
      const avgAccuracy = (
        result.accuracy_metrics.activity_detection.accuracy +
        result.accuracy_metrics.app_detection.accuracy
      ) / 2;

      totalAccuracy += avgAccuracy * result.tests_run;
      totalLatency += result.performance_metrics.average_latency_ms * result.tests_run;
      totalTests += result.tests_run;

      return {
        model: modelName,
        accuracy: avgAccuracy,
        latency: result.performance_metrics.average_latency_ms,
        composite_score: avgAccuracy * 0.7 + (1 / result.performance_metrics.average_latency_ms) * 1000 * 0.3
      };
    });

    // Sort by composite score
    modelPerformance.sort((a, b) => b.composite_score - a.composite_score);

    metrics.total_tests = totalTests;
    metrics.average_accuracy = totalAccuracy / totalTests;
    metrics.average_latency = totalLatency / totalTests;
    metrics.best_performing_model = modelPerformance[0].model;
    metrics.model_rankings = modelPerformance;

    return metrics;
  }

  async performRealTimeValidation(samples) {
    // Simulate real-time validation against live system
    return {
      validation_type: 'real_time_comparison',
      samples_tested: samples.length,
      live_system_correlation: 0.85 + Math.random() * 0.1,
      latency_overhead_ms: 50 + Math.random() * 100,
      validation_accuracy: 0.90 + Math.random() * 0.08
    };
  }

  async generatePerformanceInsights(results) {
    return {
      key_findings: [
        `Best accuracy model: ${results.overall_metrics.best_performing_model}`,
        `Average system accuracy: ${(results.overall_metrics.average_accuracy * 100).toFixed(1)}%`,
        `Average response time: ${results.overall_metrics.average_latency.toFixed(0)}ms`
      ],
      improvement_opportunities: [
        'Consider ensemble methods for improved accuracy',
        'Implement confidence-based routing to optimal models',
        'Add specialized models for specific activity types'
      ],
      production_readiness: {
        accuracy_threshold_met: results.overall_metrics.average_accuracy > 0.8,
        latency_threshold_met: results.overall_metrics.average_latency < 2000,
        confidence_calibration: 'requires_adjustment',
        recommended_deployment: results.overall_metrics.best_performing_model
      }
    };
  }

  // Public API methods
  async getBacktestResults(backtestId) {
    const results = this.backtestResults.get(backtestId);
    return results ? { success: true, data: results } : { success: false, error: 'Backtest not found' };
  }

  async listBacktests() {
    const backtests = Array.from(this.backtestResults.entries()).map(([id, result]) => ({
      backtest_id: id,
      started_at: result.started_at,
      completed_at: result.completed_at,
      total_tests: result.test_configuration.sample_size * Object.keys(result.model_results).length,
      overall_accuracy: result.overall_metrics.average_accuracy
    }));

    return { success: true, data: backtests };
  }

  async getPerformanceMetrics() {
    return {
      success: true,
      data: this.performanceMetrics
    };
  }

  /**
   * Analyze confidence correlation with accuracy
   */
  async analyzeConfidenceCorrelation(modelResults) {
    const analysis = {
      correlation_strength: {},
      confidence_calibration: {},
      overconfidence_patterns: {},
      recommendations: []
    };

    Object.entries(modelResults).forEach(([modelName, results]) => {
      const testResults = results.test_results || [];

      if (testResults.length === 0) {
        analysis.correlation_strength[modelName] = 0;
        analysis.confidence_calibration[modelName] = 'insufficient_data';
        return;
      }

      // Calculate confidence vs accuracy correlation
      const confidences = testResults.map(test => test.confidence || 0.5);
      const accuracies = testResults.map(test => test.accuracy_scores?.overall_score || 0);

      const correlation = this.calculateCorrelation(confidences, accuracies);
      analysis.correlation_strength[modelName] = correlation;

      // Analyze calibration by binning confidence scores
      const calibrationBins = this.calculateCalibration(confidences, accuracies);
      analysis.confidence_calibration[modelName] = calibrationBins;

      // Detect overconfidence patterns
      const overconfident = testResults.filter(test =>
        (test.confidence || 0.5) > 0.8 && (test.accuracy_scores?.overall_score || 0) < 0.6
      );
      analysis.overconfidence_patterns[modelName] = {
        overconfident_predictions: overconfident.length,
        overconfidence_rate: overconfident.length / testResults.length,
        avg_confidence_when_wrong: overconfident.reduce((sum, test) =>
          sum + (test.confidence || 0), 0) / (overconfident.length || 1)
      };
    });

    // Generate recommendations
    Object.entries(analysis.correlation_strength).forEach(([model, correlation]) => {
      if (correlation < 0.3) {
        analysis.recommendations.push({
          model,
          type: 'confidence_calibration',
          severity: 'high',
          message: 'Poor confidence-accuracy correlation detected',
          action: 'Implement confidence calibration techniques'
        });
      }
    });

    return analysis;
  }

  /**
   * Calculate Pearson correlation coefficient
   */
  calculateCorrelation(x, y) {
    if (x.length !== y.length || x.length === 0) return 0;

    const n = x.length;
    const sumX = x.reduce((a, b) => a + b, 0);
    const sumY = y.reduce((a, b) => a + b, 0);
    const sumXY = x.reduce((sum, xi, i) => sum + xi * y[i], 0);
    const sumX2 = x.reduce((sum, xi) => sum + xi * xi, 0);
    const sumY2 = y.reduce((sum, yi) => sum + yi * yi, 0);

    const numerator = n * sumXY - sumX * sumY;
    const denominator = Math.sqrt((n * sumX2 - sumX * sumX) * (n * sumY2 - sumY * sumY));

    return denominator === 0 ? 0 : numerator / denominator;
  }

  /**
   * Calculate confidence calibration bins
   */
  calculateCalibration(confidences, accuracies) {
    const bins = {
      '0.0-0.2': { predictions: 0, correct: 0, avg_confidence: 0 },
      '0.2-0.4': { predictions: 0, correct: 0, avg_confidence: 0 },
      '0.4-0.6': { predictions: 0, correct: 0, avg_confidence: 0 },
      '0.6-0.8': { predictions: 0, correct: 0, avg_confidence: 0 },
      '0.8-1.0': { predictions: 0, correct: 0, avg_confidence: 0 }
    };

    confidences.forEach((conf, i) => {
      const accuracy = accuracies[i];
      let binKey;

      if (conf < 0.2) binKey = '0.0-0.2';
      else if (conf < 0.4) binKey = '0.2-0.4';
      else if (conf < 0.6) binKey = '0.4-0.6';
      else if (conf < 0.8) binKey = '0.6-0.8';
      else binKey = '0.8-1.0';

      bins[binKey].predictions++;
      bins[binKey].avg_confidence += conf;
      if (accuracy > 0.7) bins[binKey].correct++;
    });

    // Calculate averages
    Object.values(bins).forEach(bin => {
      if (bin.predictions > 0) {
        bin.avg_confidence /= bin.predictions;
        bin.accuracy = bin.correct / bin.predictions;
      }
    });

    return bins;
  }
}

module.exports = AIAnalysisBacktestService;